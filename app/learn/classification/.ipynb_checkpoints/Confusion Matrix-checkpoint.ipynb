{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a44c82d",
   "metadata": {},
   "source": [
    "# <font color=\"orange\">Confusion Matrix</font>\n",
    "\n",
    "<p>A confusion matrix is a table that is often used to <strong>describe the performance of a classification model</strong> (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand.</p>\n",
    "\n",
    "<img src=\"../../img/confusion_matrix_simple2.png\">\n",
    "\n",
    "<p>What can we learn from this matrix?</p>\n",
    "<ul>\n",
    "<li>There are two possible predicted classes: \"yes\" and \"no\". If we were predicting the presence of a disease, for example, \"yes\" would mean they have the disease, and \"no\" would mean they don't have the disease.</li>\n",
    "<li>The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease).</li>\n",
    "<li>Out of those 165 cases, the classifier predicted \"yes\" 110 times, and \"no\" 55 times.</li>\n",
    "<li>In reality, 105 patients in the sample have the disease, and 60 patients do not.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Let's now define the most basic terms, which are whole numbers (not rates):</p>\n",
    "<ul>\n",
    "<li><strong>true positives (TP):</strong> These are cases in which we predicted yes (they have the disease), and they do have the disease.</li>\n",
    "<li><strong>true negatives (TN):</strong> We predicted no, and they don't have the disease.</li>\n",
    "<li><strong>false positives (FP):</strong> We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")</li>\n",
    "<li><strong>false negatives (FN):</strong> We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p>Let's now define the most basic terms, which are whole numbers (not rates):</p>\n",
    "<img src=\"../../img/confusion_matrix2.png\">\n",
    "\n",
    "<ul>\n",
    "<li><strong>Accuracy:</strong> Overall, how often is the classifier correct?\n",
    "<ul>\n",
    "<li>(TP+TN)/total = (100+50)/165 = 0.91</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Misclassification Rate:</strong> Overall, how often is it wrong?\n",
    "<ul>\n",
    "<li>(FP+FN)/total = (10+5)/165 = 0.09</li>\n",
    "<li>equivalent to 1 minus Accuracy</li>\n",
    "<li>also known as \"Error Rate\"</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>True Positive Rate:</strong> When it's actually yes, how often does it predict yes?\n",
    "<ul>\n",
    "<li>TP/actual yes = 100/105 = 0.95</li>\n",
    "<li>also known as \"Sensitivity\" or \"Recall\"</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>False Positive Rate:</strong> When it's actually no, how often does it predict yes?\n",
    "<ul>\n",
    "<li>FP/actual no = 10/60 = 0.17</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>True Negative Rate:</strong> When it's actually no, how often does it predict no?\n",
    "<ul>\n",
    "<li>TN/actual no = 50/60 = 0.83</li>\n",
    "<li>equivalent to 1 minus False Positive Rate</li>\n",
    "<li>also known as \"Specificity\"</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Precision:</strong> When it predicts yes, how often is it correct?\n",
    "<ul>\n",
    "<li>TP/predicted yes = 100/110 = 0.91</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><strong>Prevalence:</strong> How often does the yes condition actually occur in our sample?\n",
    "<ul>\n",
    "<li>actual yes/total = 105/165 = 0.64</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "<p>A couple other terms are also worth mentioning:</p>\n",
    "\n",
    "<ul>\n",
    "<li><strong>Null Error Rate:</strong> This is how often you would be wrong if you always predicted the majority class. (In our example, the null error rate would be 60/165=0.36 because if you always predicted yes, you would only be wrong for the 60 \"no\" cases.) This can be a useful baseline metric to compare your classifier against. However, the best classifier for a particular application will sometimes have a higher error rate than the null error rate, as demonstrated by the <a href=\"http://en.wikipedia.org/wiki/Accuracy_paradox\">Accuracy Paradox</a>.</li>\n",
    "<li><strong>Cohen's Kappa:</strong> This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. In other words, a model will have a high Kappa score if there is a big difference between the accuracy and the null error rate. (<a href=\"http://en.wikipedia.org/wiki/Cohen's_kappa\">More details about Cohen's Kappa.</a>)</li>\n",
    "<li><strong>F Score:</strong> This is a weighted average of the true positive rate (recall) and precision. (<a href=\"http://en.wikipedia.org/wiki/F1_score\">More details about the F Score.</a>)</li>\n",
    "<li><strong>ROC Curve:</strong> This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. (<a href=\"https://www.dataschool.io/roc-curves-and-auc-explained/\">More details about ROC Curves.</a>)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<img src=\"../../img/1_BT3awaBdZHsit5s41LPb9A.png\">\n",
    "<img src=\"../../img/1_QRIZDkk_FffXKs_07ZlhZw.png\">\n",
    "<img src=\"../../img/1_98FaAKfPWo-EBTbjsxm4GA.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa203e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508bbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa494f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f6ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7e803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107ad08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc9786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
